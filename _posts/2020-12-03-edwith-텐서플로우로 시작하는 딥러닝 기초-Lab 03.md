---
layout: post
title: "edwith-텐서플로우로 시작하는 딥러닝 기초-Lab 03"
date: 2020-12-03
excerpt: "텐서플로우 비용함수 최소화"
tags: [edwith,스터디,tensorflow,Part-1 Basic ML]
comments: true
---

# 텐서플로우 비용함수 최소화

간략화된 가설함수 , 비용함수 수식은 다음과 같습니다. m의값은 영향을 주지 않습니다.

![image](https://user-images.githubusercontent.com/44061558/100985208-0d05a000-358f-11eb-91bf-fdfb76c56762.png)

예측값과 차이값의 제곱을 정의합니다. 비용함수는 제곱 함수이므로 다음과 같이 그려집니다.

![image](https://user-images.githubusercontent.com/44061558/100986152-2bb86680-3590-11eb-886a-6cabd86cfacb.png)


## 1. cost function in pure Python (비용 함수)
``` py
#  Lab 03: Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기
import tensorflow as tf
import numpy as np
X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])
# cost function
def cost_func(W, X, Y):
    hypothesis = X * W
    return tf.reduce_mean(tf.square(hypothesis - Y))

# -3~5 사이 구간을 15로 쪼갠다.
W_values = np.linspace(-3, 5, num=15)
cost_values = []

for feed_W in W_values:
    # W값의 따른 cost 구성
    curr_cost = cost_func(feed_W, X, Y)
    # 나온 값을 value 값에 저장한다. 
    cost_values.append(curr_cost)
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))

```
결과는 다음과 같이 나옵니다. 

![image](https://user-images.githubusercontent.com/44061558/100985402-49390080-358f-11eb-9764-e1e6b2b91e3f.png)

W값이 1에 가까워 질수록 오차가 줄어드는 것을 알 수 있습니다.



## gradient descent (경사 하강법)

손실(cost)를 최소화 하는 방법을 아는것 = 경사하강법 변수가 여러개일때도 사용이 가능합니다.

W(cost)가 줄어들 수 있는 방향으로 a,b값을 변화 시킵니다. 최소점에 도달했다 생각할때 까지 이 과정을 반복합니다.

gradient descent는 기울기가 0인 지점이 여러군대일 경우는 사용할 수 없습니다. 따라서 cost함수가 볼록 함수일 경우 사용이 가능합니다.

![image](https://user-images.githubusercontent.com/44061558/100986484-99fd2900-3590-11eb-9242-04b14a09894e.png)



``` py
import tensorflow as tf
import numpy as np
X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])
# cost value를 미분해서 구한 미분값
X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])
tf.random.set_seed(0)
x_data = [1., 2., 3., 4.]
y_data = [1., 3., 5., 7.]
# 정규분포 함수를 만듦
# W는 어떤값이던 0으로 수렴해 가게 된다.
#W = tf.Variable(tf.random.normal([1], -100., 100.))
W = tf.Variable([5.0])
for step in range(300):
    hypothesis = W * X
    # 차이 제곱의 평균
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
    alpha = 0.01
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
    descent = W - tf.multiply(alpha, gradient)
    W.assign(descent)

    if step % 10 == 0:
        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))
``` 
결과는 다음과 같이 나옵니다. W가 특정값(cost가 0)으로 수렴해 가는것을 알 수 있습니다. 

![image](https://user-images.githubusercontent.com/44061558/100985750-ac2a9780-358f-11eb-801a-64e3f40a694c.png)
